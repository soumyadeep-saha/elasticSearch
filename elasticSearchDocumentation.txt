Documentation
New
https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html
https://www.elastic.co/blog/found-elasticsearch-top-down
<<<<https://hub.packtpub.com/how-does-elasticsearch-work-tutorial/>>>>
Old
https://www.elastic.co/guide/en/elasticsearch/reference/1.4/index.html

You can monitor shard sizes using the _cat/shards api

Installation
cd D:\tools\elasticsearch-6.4.3\bin
And now we are ready to start our node and single cluster (Windows users should run the elasticsearch.bat file):
elasticsearch
If everything goes well, you should see a bunch of messages that look like below:
[2016-09-16T14:17:56,748][INFO ][o.e.n.Node               ] [6-bjhwl] started
[2018-11-12T17:02:28,131][INFO ][o.e.n.Node               ] [QD8_6ce] started --> QD8_6ce is the node name, name will be diferrent in my case

As mentioned previously, we can override either the cluster or node name. This can be done from the command line when starting Elasticsearch as follows:
elasticsearch -Ecluster.name=my_cluster_name -Enode.name=my_node_name
By default, Elasticsearch uses port 9200 to provide access to its REST API. This port is configurable if necessary.
http://127.0.0.1:9200/  or http://localhost:9200/

Elasticsearch is powered by Lucene, a powerful open-source full-text search library, under the hood.
The relationship between Elasticsearch and Lucene, is like that of the relationship between a car and its engine
All the data in Elasticsearch is internally stored in  Apache Lucene as an inverted index. Although data is stored in Apache Lucene, Elasticsearch is what makes it distributed and provides the easy-to-use APIs

RDBMS		Elasticsearch
Database	Index
Table		Type
Row			Document


Cluster Health
To check the cluster health, we will be using the _cat API
http://localhost:9200/_cat/master?v
We can also get a list of nodes in our cluster as follows:
http://localhost:9200/_cat/nodes?v
Now let’s take a peek at our indices:
http://localhost:9200/_cat/indices?v
The command creates the index named "customer" using the PUT verb. We simply append pretty to the end of the call to tell it to pretty-print the JSON response (if any)
PUT /customer?pretty  http://localhost:9200/customer?pretty
{
    "acknowledged": true,
    "shards_acknowledged": true,
    "index": "customer"
}
GET /_cat/indices?v   http://localhost:9200/_cat/indices?v
The ?v at the end of the URL tells Elasticsearch to return the headers of the data it's returning. It's not required, but it helps explain the data

Index And Query a document
We’ll index a simple customer document into the customer index, with an ID of 1 as follows:
Without creating index "customer", we can directly create a document
PUT /customer/_doc/1?pretty  http://localhost:9200/customer/_doc/1?pretty
{
  "name": "John Doe"
}
And the response:
{
  "_index" : "customer",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
Retrieve thea above document 
GET /customer/_doc/1?pretty  http://localhost:9200/customer/_doc/1?pretty
{
    "_index": "customer",
    "_type": "_doc",
    "_id": "1",
    "_version": 1,
    "found": true,
    "_source": {
        "name": "John Doe"
    }
}

Delete an index
Delete the index that we just created and then list all the indexes again:
DELETE /customer?pretty  http://localhost:9200/customer?pretty
{
    "acknowledged": true
}
GET /_cat/indices?v  http://localhost:9200/_cat/indices?v

Pattern of how we access data in Elasticsearch summarized as follows:
<HTTP Verb> /<Index>/<Type>/<ID>

Modify your data
By default, you can expect a one second delay (refresh interval) from the time you index/update/delete your data until the time that it appears in your search results. This is an important distinction from other platforms like SQL wherein data is immediately available after a transaction is completed
Again, the above will index the specified document into the customer index, with the ID of 1. If we then executed the above command again with a different (or same) document, Elasticsearch will replace (i.e. reindex) a new document on top of the existing one with the ID of 1:
PUT /customer/_doc/1?pretty  http://localhost:9200/customer1/_doc/1?pretty
{
  "name": "Soumyadeep Saha"
}

Mapping that Elasticsearch generated by using the _mapping API
http://localhost:9200/acme-production/_mapping
Settings that Elasticsearch generated by using the _settings API
http://localhost:9200/acme-production/_settings

You can see it in the search results with the _search endpoint
http://localhost:9200/acme-production/_search?pretty

Updating documents
Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot
POST /customer/_doc/1/_update?pretty
http://localhost:9200/customer1/_doc/1/_update?pretty
{
	"doc": {
		"name": "Soumyadeep Saha",
		"age": 28
	}
}
http://localhost:9200/customer1/_doc/1?pretty

Deleting a document is fairly straightforward. This example shows how to delete our previous customer with the ID of 2:
DELETE /customer/_doc/2?pretty
http://localhost:9200/customer1/_doc/1?pretty

Batch processing
In addition to being able to index, update, and delete individual documents, Elasticsearch also provides the ability to perform any of the above operations in batches using the _bulk API.This functionality is important in that it provides a very efficient mechanism to do multiple operations as fast as possible with as few network roundtrips as possible.
The following call indexes two documents (ID 1 - John Doe and ID 2 - Jane Doe) in one bulk operation
POST /customer/_doc/_bulk?pretty  http://localhost:9200/customer1/_doc/_bulk?pretty
{"index":{"_id":"1"}}
{"name": "Soumyadeep Saha1" }
{"index":{"_id":"2"}}
{"name": "Soumyadeep Saha2" }
This example updates the first document (ID of 1) and then deletes the second document (ID of 2) in one bulk operation:
POST /customer/_doc/_bulk?pretty  http://localhost:9200/customer1/_doc/_bulk?pretty
{"update":{"_id":"1"}}
{"doc": { "name": "Soumyadeep Saha1 becomes Soumyadeep Saha2" } }
{"delete":{"_id":"2"}}

Sample dataset
POST  http://localhost:9200/bank/_doc?pretty
{
	"account_number": 0,
	"balance": 16623,
	"firstname": "Bradshaw",
	"lastname": "Mckenzie",
	"age": 29,
	"gender": "F",
	"address": "244 Columbus Place",
	"employer": "Euron",
	"email": "bradshawmckenzie@euron.com",
	"city": "Hobucken",
	"state": "CO"
}
GET  http://localhost:9200/bank/_doc/k1IqDWcBcwJjXwTzRBwF?pretty
GET  http://localhost:9200/_cat/indices?v
You can download the sample dataset (accounts.json) from here. Extract it to our current directory and let’s load it into our cluster as follows:

curl -H "Content-Type: application/json" -XPOST "localhost:9200/bank/_doc/_bulk?pretty&refresh" --data-binary "@accounts.json"
curl "localhost:9200/_cat/indices?v"
Which means that we just successfully bulk indexed 1000 documents into the bank index (under the _doc type).

The Search API
There are two basic ways to run searches: one is by sending search parameters through the REST request URI and the other by sending them through the REST request body. The request body method allows you to be more expressive and also to define your searches in a more readable JSON format. We’ll try one example of the request URI method but for the remainder of this tutorial, we will exclusively be using the request body method
The REST API for search is accessible from the _search endpoint. This example returns all documents in the bank index:
GET /bank/_search?q=*&sort=account_number:asc&pretty
http://localhost:9200/bank/_search?q=*&sort=account_number:asc&pretty
Search with a specific string
We are searching (_search endpoint) in the bank index, and the q=* parameter instructs Elasticsearch to match all documents in the index. The sort=account_number:asc parameter indicates to sort the results using the account_number field of each document in an ascending order. The pretty parameter, again, just tells Elasticsearch to return pretty-printed JSON results
http://localhost:9200/bank/_search?q=*99*&sort=account_number:asc&pretty

Query Language
Query DSL (Domain Specific Language)
The query part tells us what our query definition is and the match_all part is simply the type of query that we want to run. The match_all query is simply a search for all documents in the specified index
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": { "match_all": {} }
}
We also can pass other parameters to influence the search results
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": { "match_all": {} },
  "size": 1
}
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": { "match_all": {} },
  "from": 10,
  "size": 10
}
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": { "match_all": {} },
  "sort": { "balance": { "order": "desc" } }
}

Executing searches
By default, the full JSON document is returned as part of all searches. This is referred to as the source (_source field in the search hits). If we don’t want the entire source document returned, we have the ability to request only a few fields from within source to be returned
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": { "match_all": {} },
  "_source": ["account_number", "balance"]
}
Note that the above example simply reduces the _source field. It will still only return one field named _source but within it, only the fields account_number and balance are included, similar in concept to the SQL SELECT FROM field list

Let’s now introduce a new query called the match query, which can be thought of as a basic fielded search query (i.e. a search done against a specific field or set of fields)
This example returns the account numbered 20
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": { "match": { "account_number": 3 } }
}
This example returns all accounts containing the term "mill" in the address
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": { "match": { "address": "mill" } }
}
This example returns all accounts containing the term "mill" or "lane" in the address
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": { "match": { "address": "mill lane" } }
}
This example is a variant of match (match_phrase) that returns all accounts containing the phrase "mill lane" in the address
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": { "match_phrase": { "address": "mill lane" } }
}
Let’s now introduce the bool query. The bool query allows us to compose smaller queries into bigger queries using boolean logic.
This example composes two match queries and returns all accounts containing "mill" and "lane" in the address
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "address": "Place" } },
        { "match": { "address": "Place" } }
      ]
    }
  }
}
In the above example, the bool "must" clause specifies all the queries that must be true for a document to be considered a match.

In contrast, this example composes two match queries and returns all accounts containing "mill" or "lane" in the address:
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "address": "Place" } },
        { "match": { "address": "Place1" } }
      ]
    }
  }
}
In the above example, the bool "should" clause specifies a list of queries either of which must be true for a document to be considered a match

This example composes two match queries and returns all accounts that contain neither "mill" nor "lane" in the address
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": {
    "bool": {
      "must_not": [
		{ "match": { "address": "Place11" } },
        { "match": { "address": "Place1" } }
      ]
    }
  }
}
In the above example, the bool must_not clause specifies a list of queries none of which must be true for a document to be considered a match.

We can combine must, should, and must_not clauses simultaneously inside a bool query. Furthermore, we can compose bool queries inside any of these bool clauses to mimic any complex multi-level boolean logic.
This example returns all accounts of anybody who is 40 years old but doesn’t live in ID(aho):
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "age": "40" } }
      ],
      "must_not": [
        { "match": { "state": "ID" } }
      ]
    }
  }
}

Executing filters
The score is a numeric value that is a relative measure of how well the document matches the search query that we specified. The higher the score, the more relevant the document is, the lower the score, the less relevant the document is.
But queries do not always need to produce scores, in particular when they are only used for "filtering" the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.
The bool query that we introduced in the previous section also supports filter clauses which allow to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let’s introduce the range query, which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.
This example uses a bool query to return all accounts with balances between 20000 and 30000, inclusive. In other words, we want to find accounts with a balance that is greater than or equal to 20000 and less than or equal to 30000
GET /bank/_search  http://localhost:9200/bank/_search
{
  "query": {
    "bool": {
      "must": { "match_all": {} },
      "filter": {
        "range": {
          "balance": {
            "gte": 2000,
            "lte": 30000000
          }
        }
      }
    }
  }
}
Dissecting the above, the bool query contains a match_all query (the query part) and a range query (the filter part). We can substitute any other queries into the query and the filter parts. In the above case, the range query makes perfect sense since documents falling into the range all match "equally", i.e., no document is more relevant than another

Executing aggregations
https://www.elastic.co/guide/en/elasticsearch/reference/current/_executing_aggregations.html
Aggregations provide the ability to group and extract statistics from your data. The easiest way to think about aggregations is by roughly equating it to the SQL GROUP BY and the SQL aggregate functions. In Elasticsearch, you have the ability to execute searches returning hits and at the same time return aggregated results separate from the hits all in one response. This is very powerful and efficient in the sense that you can run queries and multiple aggregations and get the results back of both (or either) operations in one shot avoiding network roundtrips using a concise and simplified API.
To start with, this example groups all the accounts by state, and then returns the top 10 (default) states sorted by count descending (also default):
GET /bank/_search  http://localhost:9200/bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      }
    }
  }
}
In SQL, the above aggregation is similar in concept to:
SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESC LIMIT 10;

We can see that there are 27 accounts in ID (Idaho), followed by 27 accounts in TX (Texas), followed by 25 accounts in AL (Alabama), and so forth.
Note that we set size=0 to not show search hits because we only want to see the aggregation results in the response.
Building on the previous aggregation, this example calculates the average account balance by state (again only for the top 10 states sorted by count in descending order):
GET /bank/_search  http://localhost:9200/bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}

Configuring Elasticsearch
The configuration files should contain settings which are node-specific (such as node.name and paths), or settings which a node requires in order to be able to join a cluster, such as cluster.name and network.host.
Elasticsearch has three configuration files:
elasticsearch.yml for configuring Elasticsearch
jvm.options for configuring Elasticsearch JVM settings
log4j2.properties for configuring Elasticsearch logging
For the archive distributions, the config directory location defaults to $ES_HOME/config. The location of the config directory can be changed via the ES_PATH_CONF environment variable as follows:
ES_PATH_CONF=/path/to/my/config ./bin/elasticsearch
The configuration format is YAML. Here is an example of changing the path of the data and logs directories:
path:
    data: /var/lib/elasticsearch
    logs: /var/log/elasticsearch
Settings can also be flattened as follows:
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch

Setting JVM options
You should rarely need to change Java Virtual Machine (JVM) options. If you do, the most likely change is setting the heap size.
The preferred method of setting JVM options (including system properties and JVM flags) is via the jvm.options configuration file. The default location of this file is config/jvm.options (when installing from the tar or zip distributions) and /etc/elasticsearch/jvm.options (when installing from the Debian or RPM packages)
lines consisting of whitespace only are ignored
lines beginning with # are treated as comments and are ignored
# this is a comment
lines beginning with a - are treated as a JVM option that applies independent of the version of the JVM
-Xmx2g
lines beginning with a number followed by a : followed by a - are treated as a JVM option that applies only if the version of the JVM matches the number
8:-Xmx2g
lines beginning with a number followed by a - followed by a : are treated as a JVM option that applies only if the version of the JVM is greater than or equal to the number
8-:-Xmx2g
lines beginning with a number followed by a - followed by a number followed by a : are treated as a JVM option that applies only if the version of the JVM falls in the range of the two numbers
8-9:-Xmx2g
all other lines are rejected

Secure Settings
ome settings are sensitive, and relying on filesystem permissions to protect their values is not sufficient. For this use case, Elasticsearch provides a keystore and the elasticsearch-keystore tool to manage the settings in the keystore
Creating the keystoreedit
To create the elasticsearch.keystore, use the create command:
bin/elasticsearch-keystore create
A list of the settings in the keystore is available with the list command:
bin/elasticsearch-keystore list
Sensitive string settings, like authentication credentials for cloud plugins, can be added using the add command:
bin/elasticsearch-keystore add the.setting.name.to.set
The tool will prompt for the value of the setting. To pass the value through stdin, use the --stdin flag:
cat /file/containing/setting/value | bin/elasticsearch-keystore add --stdin the.setting.name.to.set
To remove a setting from the keystore, use the remove command:
bin/elasticsearch-keystore remove the.setting.name.to.remove

Logging configuration
Elasticsearch uses Log4j 2 for logging. Log4j 2 can be configured using the log4j2.properties file. Elasticsearch exposes three properties, ${sys:es.logs.base_path}, ${sys:es.logs.cluster_name}, and ${sys:es.logs.node_name} (if the node name is explicitly set via node.name) that can be referenced in the configuration file to determine the location of the log files
appender.rolling.type = RollingFile 
appender.rolling.name = rolling
appender.rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}.log 
appender.rolling.layout.type = PatternLayout
appender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %.-10000m%n
appender.rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}-%d{yyyy-MM-dd}-%i.log.gz 
appender.rolling.policies.type = Policies
appender.rolling.policies.time.type = TimeBasedTriggeringPolicy 
appender.rolling.policies.time.interval = 1 
appender.rolling.policies.time.modulate = true 
appender.rolling.policies.size.type = SizeBasedTriggeringPolicy 
appender.rolling.policies.size.size = 256MB 
appender.rolling.strategy.type = DefaultRolloverStrategy
appender.rolling.strategy.fileIndex = nomax
appender.rolling.strategy.action.type = Delete 
appender.rolling.strategy.action.basepath = ${sys:es.logs.base_path}
appender.rolling.strategy.action.condition.type = IfFileName 
appender.rolling.strategy.action.condition.glob = ${sys:es.logs.cluster_name}-* 
appender.rolling.strategy.action.condition.nested_condition.type = IfAccumulatedFileSize 
appender.rolling.strategy.action.condition.nested_condition.exceeds = 2GB 

Configure the RollingFile appender
Log to /var/log/elasticsearch/production.log
Roll logs to /var/log/elasticsearch/production-yyyy-MM-dd-i.log; logs will be compressed on each roll and i will be incremented
Use a time-based roll policy
Roll logs on a daily basis
Align rolls on the day boundary (as opposed to rolling every twenty-four hours)
Using a size-based roll policy
Roll logs after 256 MB
Use a delete action when rolling logs
Only delete logs matching a file pattern
The pattern is to only delete the main logs
Only delete if we have accumulated too many compressed logs
The size condition on the compressed logs is 2 GB

Important Elasticsearch configuration
path.data and path.logs
In production use, you will almost certainly want to change the locations of the data and log folder:
path:
  logs: /var/log/elasticsearch
  data: /var/data/elasticsearch
  
cluster.name
A node can only join a cluster when it shares its cluster.name with all the other nodes in the cluster. The default name is elasticsearch, but you should change it to an appropriate name which describes the purpose of the cluster.
cluster.name: logging-prod

node.name
By default, Elasticsearch will use the first seven characters of the randomly generated UUID as the node id. Note that the node id is persisted and does not change when a node restarts and therefore the default node name will also not change.
It is worth configuring a more meaningful name which will also have the advantage of persisting after restarting the node:
node.name: prod-data-2

network.host
By default, Elasticsearch binds to loopback addresses only — e.g. 127.0.0.1 and [::1]. This is sufficient to run a single development node on a server. In order to form a cluster with nodes on other servers, your node will need to bind to a non-loopback address. While there are many network settings, usually all you need to configure is network.host:
network.host: 192.168.1.10

Discovery settings
Elasticsearch uses a custom discovery implementation called "Zen Discovery" for node-to-node clustering and master election. There are two important discovery settings that should be configured before going to production.
discovery.zen.ping.unicast.hosts:
   - 192.168.1.10:9300
   - 192.168.1.11 
   - seeds.mydomain.com 
The port will default to transport.profiles.default.port and fallback to transport.tcp.port if not specified.
A hostname that resolves to multiple IP addresses will try all resolved addresses.
To avoid a split brain, this setting should be set to a quorum of master-eligible nodes:
(master_eligible_nodes / 2) + 1
In other words, if there are three master-eligible nodes, then minimum master nodes should be set to (3 / 2) + 1 or 2:
discovery.zen.minimum_master_nodes: 2

Setting the heap size
By default, Elasticsearch tells the JVM to use a heap with a minimum and maximum size of 1 GB. When moving to production, it is important to configure heap size to ensure that Elasticsearch has enough heap available.
Elasticsearch will assign the entire heap specified in jvm.options via the Xms (minimum heap size) and Xmx (maximum heap size) 
-Xms2g 
-Xmx2g 
Set the minimum heap size to 2g.
Set the maximum heap size to 2g.

JVM heap dump path
By default, Elasticsearch configures the JVM to dump the heap on out of memory exceptions to the default data directory (this is /var/lib/elasticsearch for the RPM and Debian package distributions, and the data directory under the root of the Elasticsearch installation for the tar and zip archive distributions)

GC logging
By default, Elasticsearch enables GC logs. These are configured in jvm.options and default to the same default location as the Elasticsearch logs. The default configuration rotates the logs every 64 MB and can consume up to 2 GB of disk space

Temp directory
By default, Elasticsearch uses a private temporary directory that the startup script creates immediately below the system temporary directory

JVM fatal error logs
These are logs produced by the JVM when it encounters a fatal error (e.g., a segmentation fault). If this path is not suitable for receiving logs, you should modify the entry -XX:ErrorFile=... in jvm.options to an alternate path

Starting Elasticsearch
.\bin\elasticsearch.exe

Stopping ElasticsearchAn orderly shutdown of Elasticsearch ensures that Elasticsearch has a chance to cleanup and close outstanding resources. For example, a node that is shutdown in an orderly fashion will remove itself from the cluster, sync translogs to disk, and perform other related cleanup activities. You can help ensure an orderly shutdown by properly stopping Elasticsearch
JVM internal error 128
Out of memory error 127
Stack overflow error 126
Unknown virtual machine error 125
Serious I/O error 124
Unknown fatal error 1

<<<<How does Elasticsearch work?>>>>
Basics of Elasticsearch, its advantages, how to install it and indexing the documents using Elasticsearch
What is Elasticsearch?
Elasticsearch is an open-source, enterprise-grade search engine which can power extremely fast searches that support all data discovery applications. With Elasticsearch we can store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying search engine that powers applications that have simple/complex search features and requirements.

Advantages of Elasticsearch
BUILT ON TOP OF LUCENE – Being built on top of Lucene, it offers the most powerful full-text search capabilities.
DOCUMENT-ORIENTED – It stores complex entities as structured JSON documents and indexes all fields by default, providing a higher performance.
SCHEMA FREE – It stores a large quantity of semi-structured (JSON) data in a distributed fashion. It also attempts to detect the data structure, index the data present and makes it search-friendly.
FULL TEXT SEARCH – Elasticsearch performs linguistic searches against documents and returns the documents that matches the search condition. Result relevancy for the given query is calculated using TF/IDF algorithm.
RESTFUL API – Elasticsearch supports REST API which is light-weight protocol. We can query Elasticsearch using the REST API with Chrome plug-in Sense. Sense provides a simple user interface. Sense plugin has features like autocomplete Elasticsearch query syntax, copying the query as cURL command.

Terminologies :
Cluster: A cluster is a collection of nodes that shares data.
Node: A node is a single server that is part of the cluster, stores the data, and participates in the cluster’s indexing and search capabilities.
Index: An index is a collection of documents with similar characteristics. An index is more equivalent to a schema in RDBMS.
Type: There can be multiple types within an index. For example, an ecommerce application can have used products in one type and new products in another type of the same index. One index can have multiple types as multiple tables in one database.
Document: A document is a basic unit of information that can be indexed. It is like a row in a table.
Shards and Replicas: Elastic Search indexes are divided into multiple pieces called shards, which allows the index to scale horizontally. Elastic Search also allows us to make copies of index shards, which are called replicas.

Usecases :
Ecommerce websites use elasticsearch to index their entire product catalog and inventory with all the product attributes with which the end user can search against.
So whenever a user search for a product in the website, the corresponding query will hit an index which has millions of products and it will retrieve the product in near real time.
You want to collect log or transaction data and want to analyze and mine this data to look for statistics, summarizations, or anomalies.
In this case, you can index this data into Elasticsearch. Once the data is in Elasticsearch, we can visualize the data in timelion/d3.js to better understand the collected logs

To confirm everything is working fine, type http://localhost:9200

Indexing Documents
Elasticsearch tends to use Lucene indexes to store & retrieve data. Adding ‘data’ to Elasticsearch is known as “indexing.” While performing an indexing operation, Elasticsearch converts raw data into its internal documents. Each document is nothing but a mere set of correlating keys and values: Here, the keys are strings and the values would be one of the numerous data types such as strings, numbers, lists, and dates, etc.
http://localhost:9200/java/soumyadeep/1?pretty
{ 
"Id" :	"1",
"Name" : "Soumyadeep",
"Country" : "India",
"Balance" : "1000000000"
}
This command will insert the JSON document into an index named 'java' with the type named 'soumyadeep'. 1 is the ID here. If we didn’t provide any ID here, it will simply create one for you. Pretty is used to pretty print the JSON response. To replace an existing document with an updated data, we just PUT it again.

In order to bulk load the data, we can use Bulk API of Elasticsearch.
curl -XPOST ‘localhost:9200/java/soumyadeep/_bulk?pretty&refresh’ –data-binary “@/home/ubuntu/Ex.json”
The above command loads the Ex.json file into the patient index.

Retrieving a Document
http://localhost:9200/java/soumyadeep/1?pretty
It returns the document with the id 1 and some metadata about the document

Deleting a Document
http://localhost:9200/java/soumyadeep/1?pretty
This command deletes the JSON document with the id 1.
In order to delete a document that matches a specific condition we can use _delete_by_query API.

Searching a document
http://localhost:9200/java/soumyadeep/_search?q=Name:soumyadeep&pretty=true
curl.exe -XGET 'http://localhost:9200/java/soumyadeep/_search?q=Name:soumyadeep&pretty=true'
curl -XGET 'http://localhost:9200/java/soumyadeep/_search?q=Name:soumyadeep&pretty=true'
Searching all documents
http://localhost:9200java/soumyadeep/_search?pretty
http://localhost:9200/_search?pretty

The _search endpoint
Now that we have put some java into our index, let's see if we can find them again by searching. In order to search with ElasticSearch we use the _search endpoint, optionally with an index and type. That is, we make requests to an URL following this pattern: <index>/<type>/_search where index and type are both optional.
In other words, in order to search for our java we can make POST requests to either of the following URLs:
http://localhost:9200/_search - Search across all indexes and all types.
http://localhost:9200/java/_search - Search across all types in the java index.
http://localhost:9200/java/soumyadeep/_search - Search explicitly for documents of type soumyadeep within the java index.
As we only have a single index and a single type which one we use doesn't matter. We'll use the first URL for the sake of brevity.
That’s how we index a document using Elasticsearch.
Be it in terms of configuration and usage, elasticsearch is quite elastic in comparison to it’s peers. Systems working with big data may encounter I/O bottlenecks due to data analysis and search operations. For systems like these, elasticsearch would be the ideal choice.

Search request body and ElasticSearch's query DSL
If we simply send a request to one of the above URL's we'll get all of our movies back. In order to make a more useful search request we also need to supply a request body with a query. The request body should be a JSON object which, among other things, can contain a property named "query" in which we can use ElasticSearch's query DSL

Basic free text search
The query DSL features a long list of different types of queries that we can use. For "ordinary" free text search we'll most likely want to use one called "query string query".
A query string query is an advanced query with a lot of different options that ElasticSearch will parse and transform into a tree of simpler queries. Still, it can be very easy to use if we ignore all of its optional parameters and simply feed it a string to search for.
{
    "query": {
        "query_string": {
            "query": "any keyword to search"
        }
    }
}

Specifying fields to search in
One such setting is called "fields" and can be used to specify a list of fields to search in. If we don't use that the query will default to searching in a special field called "_all" that ElasticSearch automatically generates based on all of the individual fields in a document.
Let's try to search for movies only by title. That is, if we search for "soumyadeep" we want to get a hit for "The promotion of soumyadeep soumyadeep by the Coward Robert Ford" but not for either of the movies directed by "Francis soumyadeep Coppola".
In order to do that we modify the previous search request body so that the query string query has a fields property with an array of fields we want to search in
{
    "query": {
        "query_string": {
            "query": "soumyadeep or any string to search for",
            "fields": ["Name"]
        }
    }
}

Filtering
{
    "query": {
        "query_string": {
            "query": "drama"
        }
    }
}
As we have five movies in our index containing the word "drama" in the _all field (from the category field) we get five hits for the above query. Now, imagine that we want to limit these hits to movies released in 1962. In order to do that we need to apply a filter requiring the "year" field to equal 1962.

To add such a filter we modify our search request body so that our current top level query, the query string query, is wrapped in a filtered query:
{
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "drama"
                }
            },
            "filter": {
                //Filter to apply to the query
            }
        }
    }
}
A filtered query is a query that has two properties, query and filter. When executed it filters the result of the query using the filter. To finalize the query we'll need to add a filter requiring the year field to have value 1962.
ElasticSearch's query DSL has a wide range of filters to choose from. For this simple case where a certain field should match a specific value a term filter will work well.
"filter": {
    "term": { "year": 1962 }
}

The complete search request now looks like this:
curl -XPOST "http://localhost:9200/_search" -d'
{
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "drama"
                }
            },
            "filter": {
                "term": { "year": 1962 }
            }
        }
    }
}

Filtering without a query
What if all we want to do is apply a filter? That is, we want all movies matching a certain criteria
One solution for doing this is to modify our current search request, replacing the query string query in the filtered query with a match_all query which is a query that simply matches everything. Like this:

curl -XPOST "http://localhost:9200/_search" -d'
{
    "query": {
        "filtered": {
            "query": {
                "match_all": {
                }
            },
            "filter": {
                "term": { "year": 1962 }
            }
        }
    }
}'
Another, simpler option is to use a constant score query:

curl -XPOST "http://localhost:9200/_search" -d'
{
    "query": {
        "constant_score": {
            "filter": {
                "term": { "year": 1962 }
            }
        }
    }
}'

Mapping
curl -XPOST "http://localhost:9200/_search" -d'
{
    "query": {
        "constant_score": {
            "filter": {
                "term": { "director": "Francis Ford Coppola" }
            }
        }
    }
}'
Answer is 0 found
As we have two movies directed by Francis Ford Coppola in our index it doesn't seem too far fetched that this request should result in two hits, right? That's not the case however
When we index a document with ElasticSearch it (simplified) does two things: it stores the original data untouched for later retrieval in the form of _source and it indexes each JSON property into one or more fields in a Lucene index. During the indexing it processes each field according to how the field is mapped. If it isn't mapped default mappings depending on the fields type (string, number etc) is used.
As we haven't supplied any mappings for our index ElasticSearch uses the default mappings for strings for the director field. This means that in the index the director fields value isn't "Francis Ford Coppola". Instead it's something more like ["francis", "ford", "coppola"].
We can verify that by modifying our filter to instead match "francis" (or "ford" or "coppola")

<<<<Elasticsearch>>>> is based on a Lucene Standard Analyser. It is a semi-structured / document-oriented database where data is stored in JSON document form.
Elastic Search is an open-source search tool that is built on Lucene but natively it is JSON + RESTful. Elastic Search provides a JSON-style domain-specific language which can be used to execute queries, and is referred as the Query-DSL
Unlike MongoDb, which is a general purpose database, ES is a distributed text search engine meant for working on large datasets in real time.
When handling large volumes of data, ES relies on sharding. Shards are single Lucene instances managed by ES. They are of two types - Primary shards, which are first to be indexed and Replica shards which are indexed after
Each index has 5 shards associated with it, by default. This aids in maintaining high level of performance during searches with high volumes of data
In practice, a combination of logstash, Kibana and Elasticsearch are used where, 
Logstash collects, parses and stores logs with ES
Kibana is then used to search and visualize the logs indexed
This trio is commonly referred to as an ELK stack
ES is often paired with other databases such as SQL & NoSQL databases

<<<<Architechture of Elasticsearch>>>>
https://hub.packtpub.com/how-does-elasticsearch-work-tutorial/
Elasticsearch is much more than just a search engine; it supports complex aggregations, geo filters, and the list goes on. Best of all, you can run all your queries at a speed you have never seen before.  Elasticsearch, like any other open source technology, is very rapidly evolving, but the core fundamentals that power Elasticsearch don’t change.

All the data in Elasticsearch is internally stored in  Apache Lucene as an inverted index. Although data is stored in Apache Lucene, Elasticsearch is what makes it distributed and provides the easy-to-use APIs.

Inverted index in Elasticsearch
Inverted index will help you understand the limitations and strengths of Elasticsearch compared with the traditional database systems out there. Inverted index at the core is how Elasticsearch is different from other NoSQL stores, such as MongoDB, Cassandra, and so on.
We can compare an inverted index to an old library catalog card system. When you need some information/book in a library, you will use the card catalog, usually at the entrance of the library, to find the book. An inverted index is similar to the card catalog. Imagine that you were to build a system like Google to search for the web pages mentioning your search keywords. We have three web pages with Yoda quotes from Star Wars, and you are searching for all the documents with the word fear.

Document1: Fear leads to anger
Document2: Anger leads to hate
Document3: Hate leads to suffering

In a library, without a card catalog to find the book you need, you would have to go to every shelf row by row, look at each book title, and see whether it’s the book you need. Computer-based information retrieval systems do the same.
Without the inverted index, the application has to go through each web page and check whether the word exists in the web page. An inverted index is similar to the following table. It is like a map with the term as a key and list of the documents the term appears in as value.

Term	Document
Fear	1
Anger	1,2
Hate	2,3
Suffering	3
Leads	1,2,3
Once we construct an index, as shown in this table, to find all the documents with the term fear is now just a lookup. Just like when a library gets a new book, the book is added to the card catalog, we keep building an inverted index as we encounter a new web page. The preceding inverted index takes care of simple use cases, such as searching for the single term. But in reality, we query for much more complicated things, and we don’t use the exact words. Now let’s say we encountered a document containing the following:

Yosemite national park may be closed for the weekend due to forecast of substantial rainfall

We want to visit Yosemite National Park, and we are looking for the weather forecast in the park. But when we query for it in the human language, we might query something like weather in yosemite or rain in yosemite. With the current approach, we will not be able to answer this query as there are no common terms between the query and the document, as shown:

Document	Query
rainfall	rain
To be able to answer queries like this and to improve the search quality, we employ various techniques such as stemming, synonyms discussed in the following sections.

Stemming
Stemming is the process of reducing a derived word into its root word. For example, rain, raining, rained, rainfall has the common root word “rain”. When a document is indexed, the root word is stored in the index instead of the actual word. Without stemming, we end up storing rain, raining, rained in the index, and search relevance would be very low. The query terms also go through the stemming process, and the root words are looked up in the index. Stemming increases the likelihood of the user finding what he is looking for. When we query for rain in yosemite, even though the document originally had rainfall, the inverted index will contain term rain. We can configure stemming in Elasticsearch using Analyzers.

Synonyms
Similar to rain and raining, weekend and sunday mean the same thing. The document might not contain Sunday, but if the information retrieval system can also search for synonyms, it will significantly improve the search quality. Human language deals with a lot of things, such as tense, gender, numbers. Stemming and synonyms will not only improve the search quality but also reduce the index size by removing the differences between similar words.

More examples:

Pen, Pen[s] -> Pen

Eat, Eating  -> Eat

Phrase search
As a user, we almost always search for phrases rather than single words. The inverted index in the previous section would work great for individual terms but not for phrases. Continuing the previous example, if we want to query all the documents with a phrase anger leads to in the inverted index, the previous index would not be sufficient. The inverted index for terms anger and leads is shown below:

Term	Document
Anger	1,2
Leads	1,2,3
From the preceding table, the words anger and leads exist both in document1 and document2. To support phrase search along with the document, we also need to record the position of the word in the document. The inverted index with word position is shown here:

Term	Document
Fear	1:1
Anger	1:3, 2:1
Hate	2:3, 3:1
Suffering	3:3
Leads	1:2, 2:2, 3:2
Now, since we have the information regarding the position of the word, we can search if a document has the terms in the same order as the query.

Term	Document
anger	1:3, 2:1
leads	1:2, 2:2
Since document2 has anger as the first word and leads as the second word, the same order as the query, document2 would be a better match than document1. With the inverted index, any query on the documents is just a simple lookup. This is just an introduction to inverted index; in real life, it’s much more complicated, but the fundamentals remain the same. When the documents are indexed into Elasticsearch, documents are processed into the inverted index.

Scalability and availability in Elasticsearch
Let’s say you want to index a billion documents; having just a single machine might be very challenging. Partitioning data across multiple machines allows Elasticsearch to scale beyond what a single machine do and support high throughput operations. Your data is split into small parts called shards. When you create an index, you need to tell Elasticsearch the number of shards you want for the index and Elasticsearch handles the rest for you. As you have more data, you can scale horizontally by adding more machines. We will go in to more details in the sections below.

There are type of shards in Elasticsearch – primary and replica. The data you index is written to both primary and replica shards. Replica is the exact copy of the primary. In case of the node containing the primary shard goes down, the replica takes over. This process is completely transparent and managed by Elasticsearch. We will discuss this in detail in the Failure Handling section below. Since primary and replicas are the exact copies, a search query can be answered by either the primary or the replica shard. This significantly increases the number of simultaneous requests Elasticsearch can handle at any point in time.

As the index is distributed across multiple shards, a query against an index is executed in parallel across all the shards. The results from each shard are then gathered and sent back to the client. Executing the query in parallel greatly improves the search performance.


